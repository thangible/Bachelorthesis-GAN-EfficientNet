{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataset_utils import edge_stratified_split\n",
    "from classification_dataset import ClassificationDataset\n",
    "from  config.parser_config import config_parser\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET EDGE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = config_parser()\n",
    "argv = '--config localtest/localtest_config'.split(' ') \n",
    "args  = parser.parse_args(argv)\n",
    "full_dataset = ClassificationDataset(\n",
    "        one_hot = False,\n",
    "        augmentation= None,\n",
    "        npz_path= args.npz_path,\n",
    "        image_path= args.image_path,\n",
    "        label_path= args.label_path,\n",
    "        size = args.size,\n",
    "        normalize= True)\n",
    "\n",
    "edge_classes = [\"Gryllteiste\",\"Schnatterente\",\"Buchfink\",\"unbestimmte Larusmöwe\",\n",
    "                    \"Schmarotzer/Spatel/Falkenraubmöwe\",\"Brandgans\",\"Wasserlinie mit Großalgen\",\n",
    "                    \"Feldlerche\",\"Schmarotzerraubmöwe\",\"Grosser Brachvogel\",\"unbestimmte Raubmöwe\",\n",
    "                    \"Turmfalke\",\"Trauerseeschwalbe\",\"unbestimmter Schwan\",\n",
    "                    \"Sperber\",\"Kiebitzregenpfeifer\",\n",
    "                    \"Skua\",\"Graugans\",\"unbestimmte Krähe\"]\n",
    "\n",
    "edge_labels = [full_dataset._get_label_from_cat(cat) for cat in edge_classes]\n",
    "\n",
    "edge_train_data, edge_test_data = edge_stratified_split(full_dataset, full_labels = full_dataset._labels, edge_labels = edge_labels,  fraction = 0.8, random_state = 0)                     \n",
    "trainloader = DataLoader(edge_train_data,\n",
    "                                batch_size=1, \n",
    "                                shuffle=True,\n",
    "                                num_workers=args.num_workers)\n",
    "\n",
    "testloader = DataLoader(edge_test_data,\n",
    "                                batch_size=1, \n",
    "                                shuffle=True,\n",
    "                                num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dict = {}\n",
    "# for _, label, cat, name in trainloader:\n",
    "#     cat = cat[0]\n",
    "#     name = name[0]\n",
    "#     if cat in train_dict.keys():\n",
    "#         train_dict[cat].append(name)\n",
    "#     else:\n",
    "#         train_dict[cat] = [name]\n",
    "        \n",
    "\n",
    "# test_dict = {}\n",
    "# for _, label, cat, name in testloader:\n",
    "#     cat = cat[0]\n",
    "#     name = name[0]\n",
    "#     if cat in test_dict.keys():\n",
    "#         test_dict[cat].append(name)\n",
    "#     else:\n",
    "#         test_dict[cat] = [name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_dir = '/Users/thang/Documents/Thang/edge_cases'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil \n",
    "# for cat in train_dict.keys():\n",
    "#     for image in train_dict[cat]:\n",
    "#         from_loc = os.path.join(args.image_path, image)        \n",
    "#         train_dir = os.path.join(to_dir, cat, 'train')\n",
    "#         if not os.path.exists(train_dir):\n",
    "#             os.makedirs(train_dir)\n",
    "#         to_loc = os.path.join(train_dir, image)\n",
    "#         shutil.copy(from_loc, train_dir)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cat in test_dict.keys():\n",
    "#     for image in test_dict[cat]:\n",
    "#         from_loc = os.path.join(args.image_path, image)        \n",
    "#         train_dir = os.path.join(to_dir, cat, 'test')\n",
    "#         if not os.path.exists(train_dir):\n",
    "#             os.makedirs(train_dir)\n",
    "#         to_loc = os.path.join(train_dir, image)\n",
    "#         shutil.copy(from_loc, train_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model.vanilla_cgan import Generator\n",
    "import torch.nn as nn\n",
    "device = torch.device('cpu')\n",
    "map_location=torch.device('cpu')\n",
    "class_num = 118\n",
    "img_size = 256\n",
    "model_dim = 512\n",
    "lr = 0.00005\n",
    "batch_size = 9\n",
    "z_size = 50\n",
    "generator_layer_size = [model_dim, model_dim*2, model_dim*4]\n",
    "generator = Generator(generator_layer_size, z_size, img_size, class_num)\n",
    "# discriminator = Discriminator(discriminator_layer_size, img_size, class_num).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "# d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_location=torch.device('cpu')\n",
    "g_checkpoint= torch.load('./saved_models/vanilla_gan/generator_TRAIN_cGAN,_dim 512,_lr _5e-05,_epochs _10000,_size _256.pt', map_location=torch.device('cpu'))\n",
    "generator.load_state_dict(g_checkpoint['model_state_dict'])\n",
    "g_optimizer.load_state_dict(g_checkpoint['optimizer_state_dict'])\n",
    "epoch = g_checkpoint['epoch']\n",
    "g_loss = g_checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "std = Variable(torch.rand(batch_size, z_size)*5).to(device)    \n",
    "z = torch.normal(mean=0, std=std)\n",
    "fake_labels = Variable(torch.LongTensor([64]*9)).to(device)\n",
    "raw_fake_images = generator(std, fake_labels)\n",
    "fake_images = torch.round(raw_fake_images*127.5 + 127.5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "def show(img):\n",
    "    npimg = (img.numpy()*255).astype(np.uint8)\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "    plt.gcf().set_dpi(500)\n",
    "\n",
    "grid = torchvision.utils.make_grid(fake_images, nrow=5, ncol = 5, padding=20)\n",
    "show(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29, 64, 9, 103, 62, 7, 90, 17, 63, 28, 106, 87, 83, 112, 71, 35, 69, 27, 102]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WASSERSTEIN GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.wasserstein_cgan import Discriminator, Generator, initialize_weights\n",
    "parser = config_parser()\n",
    "argv = '--config localtest/local_gan_config'.split(' ') \n",
    "args  = parser.parse_args(argv)\n",
    "LEARNING_RATE = args.lr\n",
    "BATCH_SIZE = args.batch_size\n",
    "IMAGE_SIZE = args.size\n",
    "CHANNELS_IMG = 3\n",
    "Z_DIM = args.latent_size\n",
    "NUM_EPOCHS = args.epochs\n",
    "FEATURES_CRITIC = args.model_dim\n",
    "FEATURES_GEN = args.model_dim\n",
    "CRITIC_ITERATIONS = 7\n",
    "WEIGHT_CLIP = 0.01\n",
    "full_dataset = ClassificationDataset(\n",
    "        one_hot = False,\n",
    "        augmentation= None,\n",
    "        npz_path= args.npz_path,\n",
    "        image_path= args.image_path,\n",
    "        label_path= args.label_path,\n",
    "        size = args.size,\n",
    "        normalize= True)\n",
    "\n",
    "edge_classes = [\"Gryllteiste\",\"Schnatterente\",\"Buchfink\",\"unbestimmte Larusmöwe\",\n",
    "                    \"Schmarotzer/Spatel/Falkenraubmöwe\",\"Brandgans\",\"Wasserlinie mit Großalgen\",\n",
    "                    \"Feldlerche\",\"Schmarotzerraubmöwe\",\"Grosser Brachvogel\",\"unbestimmte Raubmöwe\",\n",
    "                    \"Turmfalke\",\"Trauerseeschwalbe\",\"unbestimmter Schwan\",\n",
    "                    \"Sperber\",\"Kiebitzregenpfeifer\",\n",
    "                    \"Skua\",\"Graugans\",\"unbestimmte Krähe\"]\n",
    "\n",
    "edge_labels = [full_dataset._get_label_from_cat(cat) for cat in edge_classes]\n",
    "\n",
    "edge_train_data, _ = edge_stratified_split(full_dataset, full_labels = full_dataset._labels, edge_labels = edge_labels,  fraction = 0.8, random_state = 0)                     \n",
    "loader = DataLoader(edge_train_data,\n",
    "                                batch_size=BATCH_SIZE, \n",
    "                                shuffle=True,\n",
    "                                num_workers=args.num_workers)\n",
    "\n",
    "CLASS_NUM = full_dataset._get_num_classes()\n",
    "\n",
    "GEN = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN, CLASS_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_location=torch.device('cpu')\n",
    "g_checkpoint= torch.load('./saved_models/waasserstein_gan/generator_TRAIN_cGAN,_dim:128,_lr:_0.001,_epochs:_5000,_size:_256.pt', map_location=torch.device('cpu'))\n",
    "GEN.load_state_dict(g_checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(5, Z_DIM, 1, 1).to(device)\n",
    "labels = torch.zeros(5, 1, 1, 1).to(device)\n",
    "fake = GEN(noise, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.arange("
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN ADDED IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "data = pd.read_csv(args.label_path,index_col=False)[['tp','name','file']]\n",
    "data = data[data['file'].isin(full_dataset._image_names)]\n",
    "max_count = data[['file','name']].groupby('name').count().sort_values(by='name', ascending = False)\n",
    "to_dir = 'gan_generated_data'\n",
    "all_dir = os.path.join(to_dir, 'all_fake')\n",
    "if not os.path.exists(all_dir):\n",
    "        os.makedirs(all_dir)\n",
    "csv_info = []\n",
    "for label_pos in range(len(edge_labels)):\n",
    "    if True:\n",
    "        label = edge_labels[label_pos]\n",
    "        cat = edge_classes[label_pos]\n",
    "        max = math.floor(max_count[max_count.index ==cat]['file'].iloc[0]*0.8)\n",
    "        count = 0\n",
    "        if True:\n",
    "            \n",
    "            for i in range(0, 10, 3):\n",
    "                z = Variable(torch.rand(batch_size, z_size)*i).to(device) \n",
    "                fake_labels = Variable(torch.LongTensor([label]*9)).to(device)  \n",
    "                raw_fake_images = generator(z, fake_labels)\n",
    "                fake_images = torch.round(raw_fake_images*127.5 + 127.5).float()\n",
    "                s = fake_images[0].permute(1, 2, 0).detach().numpy()\n",
    "                fake_dir = os.path.join(to_dir, cat, 'fake')\n",
    "                image_name = '{}_{}.png'.format(cat, i).replace(' ', '_').replace(\"ä\", \"ae\").replace(\"ö\", \"oe\").replace(\"ü\", \"ue\").replace(\"ß\", \"ss\").replace(\"/\", \"_\")\n",
    "                cv2.imwrite(os.path.join(all_dir, image_name), s)\n",
    "                csv_info.append([image_name, cat])\n",
    "                # if not os.path.exists(fake_dir):\n",
    "                    # os.makedirs(fake_dir)\n",
    "                # cv2.imwrite(os.path.join(fake_dir, '{}_{}.png'.format(cat, i)), s)\n",
    "                # train_dir = os.path.join(to_dir, cat, 'train')\n",
    "                # cv2.imwrite(os.path.join(train_dir, '{}_{}.png'.format(cat, i)), s)\n",
    "                count += 1\n",
    "                if count >= max//2:\n",
    "                    break \n",
    "\n",
    "csv_path = os.path.join(to_dir, 'gan_info.csv')\n",
    "with open(csv_path, 'w') as f:\n",
    "    # using csv.writer method from CSV package\n",
    "    write = csv.writer(f)\n",
    "    write.writerow(['image_name', 'category'])\n",
    "    write.writerows(csv_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "count = data[['file','name']].groupby('name').count().sort_values(by='name', ascending = False)\n",
    "math.floor(count[count.index =='Gryllteiste']['file'].iloc[0]*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pd.read_csv(csv_path, index_col=False)\n",
    "a = info['image_name'].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = config_parser()\n",
    "argv = '--config localtest/localtest_config'.split(' ') \n",
    "args  = parser.parse_args(argv)\n",
    "\n",
    "import classification_dataset\n",
    "from dataset_utils import stratified_split\n",
    "full_dataset = classification_dataset.ClassificationDataset(\n",
    "        one_hot = False,\n",
    "        augmentation= None,\n",
    "        npz_path= args.npz_path,\n",
    "        image_path= args.image_path,\n",
    "        label_path= args.label_path,\n",
    "        size = args.size,\n",
    "        normalize= True)\n",
    "train_data, train_set_labels, validation_data, test_set_labels = stratified_split(dataset = full_dataset, \n",
    "                                                                                            labels = full_dataset._labels,\n",
    "                                                                                            fraction = 0.8,\n",
    "                                                                                            random_state=0)        \n",
    "    \n",
    "\n",
    "train_dataloader = DataLoader(train_data,\n",
    "                                batch_size=8, \n",
    "                                shuffle=True,\n",
    "                                num_workers=8)\n",
    "\n",
    "validation_dataloader = DataLoader(validation_data, \n",
    "                                batch_size=8, \n",
    "                                shuffle=True,\n",
    "                                num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thang/Documents/Thang/efficientnet/eff/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/Users/thang/Documents/Thang/efficientnet/eff/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/Users/thang/Documents/Thang/efficientnet/eff/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/Users/thang/Documents/Thang/efficientnet/eff/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/Users/thang/Documents/Thang/efficientnet/eff/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/Users/thang/Documents/Thang/efficientnet/eff/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/Users/thang/Documents/Thang/efficientnet/eff/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/Users/thang/Documents/Thang/efficientnet/eff/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for img, label, cat in validation_dataloader:\n",
    "        mask = sum(label==i for i in edge_labels).bool()\n",
    "        indices = torch.nonzero(mask).flatten()\n",
    "        cat  = np.array(cat)[mask]\n",
    "        img = img[indices]\n",
    "        label = label[indices]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype='<U35')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(c)[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, False, False, False, False, False, False]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = [False]*8\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([1, 2, 3, 4, 5])\n",
    "b = torch.Tensor([1, 2, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = sum(a==i for i in b).bool()\n",
    "indices = torch.nonzero(mask).flatten()\n",
    "indices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ILLUSTRATION OF GAN PICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_case_dir = '/Users/thang/Documents/Thang/edge_cases'\n",
    "edge_classes = [\"Gryllteiste\",\"Schnatterente\",\"Buchfink\",\"unbestimmte Larusmöwe\",\n",
    "                    \"Schmarotzer/Spatel/Falkenraubmöwe\",\"Brandgans\",\"Wasserlinie mit Großalgen\",\n",
    "                    \"Feldlerche\",\"Schmarotzerraubmöwe\",\"Grosser Brachvogel\",\"unbestimmte Raubmöwe\",\n",
    "                    \"Turmfalke\",\"Trauerseeschwalbe\",\"unbestimmter Schwan\",\n",
    "                    \"Sperber\",\"Kiebitzregenpfeifer\",\n",
    "                    \"Skua\",\"Graugans\",\"unbestimmte Krähe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thang/Documents/Thang/efficientnet/eff/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from torchvision.io import read_image\n",
    "dict = {}\n",
    "\n",
    "for edge_class in edge_classes:\n",
    "    for type in ['train', 'test', 'fake']:\n",
    "        dir_path = Path(edge_case_dir, edge_class, type)\n",
    "        if type == 'fake':\n",
    "            img_paths = list(dir_path.glob('*.png'))\n",
    "        else:\n",
    "            img_paths = list(dir_path.glob('*.jpg'))\n",
    "        # print(img_paths)\n",
    "        \n",
    "        to_add = []\n",
    "        for img_path in img_paths:\n",
    "            img = read_image(str(img_path))\n",
    "            img = torchvision.transforms.Resize(size = (500,500))(img)\n",
    "            to_add.append(img)\n",
    "        name = '{}_{}'.format(edge_class, type)\n",
    "        dict[name] = to_add    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "def img_grid(edge_class, dict = dict):\n",
    "    train = '{}_train'.format(edge_class)\n",
    "    test = '{}_train'.format(edge_class)\n",
    "    fake = '{}_fake'.format(edge_class)\n",
    "\n",
    "    # \n",
    "    train_pics = dict[train]\n",
    "    # print(len(train_pics))\n",
    "    test_pics = dict[test]\n",
    "    fake_pics = dict[fake]\n",
    "    # print(len(fake_pics))\n",
    "    if len(train_pics) <= 3:\n",
    "        grid_list = [train_pics[0], train_pics[1]]\n",
    "    else:\n",
    "        grid_list = [train_pics[0], train_pics[1], train_pics[2]]\n",
    "    Grid = make_grid(grid_list, nrow=4, padding=25)\n",
    "    return Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = []\n",
    "for edge_class in edge_classes:\n",
    "    grid = img_grid(edge_class)\n",
    "    grids.append(grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "def show_transform(pics):\n",
    "  rows= 3\n",
    "  fig_size = (15,10)\n",
    "  pic_title_size = fig_size[0]\n",
    "  fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=fig_size)\n",
    "  #\n",
    "  plt.imshow()\n",
    "  j = 0    \n",
    "  for pic in pics:\n",
    "    axes[i, 0].imshow(np.transpose(pic, (1,2,0)), interpolation='nearest')\n",
    "    axes[1, j].imshow(aug_img.astype(np.uint8))\n",
    "    axes[1, j].tick_params(left = False, right = False , labelleft = False ,\n",
    "                labelbottom = False, bottom = False)\n",
    "    axes[0, j].tick_params(left = False, right = False , labelleft = False ,\n",
    "                labelbottom = False, bottom = False)\n",
    "    axes[0, j].set_title(names[j], fontsize = pic_title_size, pad= pic_title_size)\n",
    "    j+=1\n",
    "  if title:\n",
    "    fig.suptitle(title, fontsize = pic_title_size*1.5)\n",
    "  return img, np.round(aug_img*0.5 + 127.5)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "efficientnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f2da8fd353b9accb7305af3972cbc182751196940ed0d5ee3a1a6f8e3dc0d34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
